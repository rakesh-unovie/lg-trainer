{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logo Detection Training Notebook\n",
    "\n",
    "This notebook contains all the code to train a logo detection model. It is a self-contained notebook that includes all the necessary code from the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow torch torchvision rembg tqdm numpy scikit-learn onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "Mount your Google Drive to access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n",
    "\n",
    "These are the utility functions for unzipping data, image manipulation, and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_data(zip_file_path, destination_dir):\n",
    "    \"\"\"\n",
    "    Unzips a zip file to a specified destination directory.\n",
    "\n",
    "    Args:\n",
    "        zip_file_path (str): The path to the zip file.\n",
    "        destination_dir (str): The path to the destination directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        raise FileNotFoundError(f\"Zip file not found at: {zip_file_path}\")\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(destination_dir)\n",
    "            print(f\"Successfully extracted {zip_file_path} to {destination_dir}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        raise ValueError(f\"Invalid zip file: {zip_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "def load_image(image_path: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Loads an image from a file path.\n",
    "\n",
    "    Args:\n",
    "        image_path: The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        A Pillow Image object.\n",
    "    \"\"\"\n",
    "    return Image.open(image_path)\n",
    "\n",
    "def save_image(image: Image.Image, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves an image to a file path.\n",
    "\n",
    "    Args:\n",
    "        image: The Pillow Image object to save.\n",
    "        save_path: The path to save the image to.\n",
    "    \"\"\"\n",
    "    image.save(save_path)\n",
    "\n",
    "def crop_background(background: Image.Image, width: int, height: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Crops a random area from the background image.\n",
    "\n",
    "    Args:\n",
    "        background: The background image.\n",
    "        width: The width of the desired crop.\n",
    "        height: The height of the desired crop.\n",
    "\n",
    "    Returns:\n",
    "        The cropped background image.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the background image is smaller than the desired crop dimensions.\n",
    "    \"\"\"\n",
    "    if background.width < width or background.height < height:\n",
    "        raise ValueError(\"Background image is smaller than the foreground image.\")\n",
    "    \n",
    "    left = random.randint(0, background.width - width)\n",
    "    top = random.randint(0, background.height - height)\n",
    "    right = left + width\n",
    "    bottom = top + height\n",
    "    \n",
    "    return background.crop((left, top, right, bottom))\n",
    "\n",
    "def composite_images(background: Image.Image, foreground: Image.Image) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Composites a foreground image onto a background image.\n",
    "    The foreground image should have an alpha channel.\n",
    "\n",
    "    Args:\n",
    "        background: The background image.\n",
    "        foreground: The foreground image.\n",
    "\n",
    "    Returns:\n",
    "        The composited image.\n",
    "    \"\"\"\n",
    "    background.paste(foreground, (0, 0), foreground)\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LogoDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "def load_data(input_dir, mask_dir):\n",
    "    \"\"\"\n",
    "    Loads images and masks from the specified directories.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the directory of training images.\n",
    "        mask_dir (str): Path to the directory of mask images.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing training and validation data splits.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    mask_path = Path(mask_dir)\n",
    "\n",
    "    image_files = sorted([p for p in input_path.glob(\"*.png\")])\n",
    "    mask_files = sorted([p for p in mask_path.glob(\"*.png\")])\n",
    "\n",
    "    if len(image_files) != len(mask_files):\n",
    "        print(f\"Warning: Mismatched number of images and masks. Found {len(image_files)} images and {len(mask_files)} masks.\")\n",
    "        # Use the intersection of filenames\n",
    "        image_names = {p.name for p in image_files}\n",
    "        mask_names = {p.name for p in mask_files}\n",
    "        common_names = sorted(list(image_names.intersection(mask_names)))\n",
    "        \n",
    "        image_files = [input_path / name for name in common_names]\n",
    "        mask_files = [mask_path / name for name in common_names]\n",
    "\n",
    "    images = [Image.open(p) for p in image_files]\n",
    "    masks = [Image.open(p) for p in mask_files]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from rembg import new_session\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define parameters\n",
    "# These were previously command line arguments\n",
    "# You can modify these values\n",
    "zip_file_path_str = \"/content/drive/My Drive/logo-trainer/data.zip\"\n",
    "output_path_str = \"/content/drive/My Drive/logo-trainer/models/u2net_logo.pth\"\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The rest of the training script\n",
    "zip_file_path = Path(zip_file_path_str)\n",
    "extracted_data_path = Path(\"data/extracted\")\n",
    "unzip_data(zip_file_path, extracted_data_path)\n",
    "\n",
    "input_dir = extracted_data_path / \"data/transparent\"\n",
    "mask_dir = extracted_data_path / \"data/output\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "X_train, X_val, y_train, y_val = load_data(input_dir, mask_dir)\n",
    "\n",
    "train_dataset = LogoDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = LogoDataset(X_val, y_val, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "session = new_session(\"u2net\")\n",
    "model = session.model\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available. Training will be on GPU.\")\n",
    "else:\n",
    "    print(\"GPU is not available. Training will be on CPU.\")\n",
    "    print(\"To use GPU in Google Colab, go to Runtime -> Change runtime type and select GPU as the hardware accelerator.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs[0], masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs[0]) > 0.5\n",
    "            \n",
    "            intersection = torch.logical_and(preds, masks).sum()\n",
    "            union = torch.logical_or(preds, masks).sum()\n",
    "            iou = intersection / union\n",
    "            total_iou += iou.item()\n",
    "    \n",
    "    avg_iou = total_iou / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Validation IoU: {avg_iou:.4f}\")\n",
    "\n",
    "output_path = Path(output_path_str)\n",
    "if not output_path.parent.exists():\n",
    "    output_path.parent.mkdir(parents=True)\n",
    "\n",
    "if output_path.exists():\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_path = output_path.with_name(f\"{output_path.stem}_{timestamp}{output_path.suffix}\")\n",
    "\n",
    "torch.save(model.state_dict(), output_path)\n",
    "print(f\"Model saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image Processing Script\n",
    "\n",
    "This script can be used to add backgrounds to transparent images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def process_images() -> None:\n",
    "    \"\"\"\n",
    "    Main function to process transparent images and add backgrounds.\n",
    "    \n",
    "    This script reads transparent images from `data/transparent`,\n",
    "    backgrounds from `data/bg-sample`, and saves the composited\n",
    "    images to `data/output`.\n",
    "    \"\"\"\n",
    "    transparent_dir = 'data/transparent'\n",
    "    bg_dir = 'data/bg-sample'\n",
    "    output_dir = 'data/output'\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    transparent_images = [f for f in os.listdir(transparent_dir) if f.endswith('.png')]\n",
    "    background_images = [f for f in os.listdir(bg_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    if not background_images:\n",
    "        logging.warning(\"No background images found.\")\n",
    "        return\n",
    "\n",
    "    for transparent_image_name in transparent_images:\n",
    "        try:\n",
    "            transparent_image_path = os.path.join(transparent_dir, transparent_image_name)\n",
    "            transparent_image = load_image(transparent_image_path)\n",
    "\n",
    "            bg_image_name = random.choice(background_images)\n",
    "            bg_image_path = os.path.join(bg_dir, bg_image_name)\n",
    "            background_image = load_image(bg_image_path)\n",
    "\n",
    "            cropped_bg = crop_background(background_image, transparent_image.width, transparent_image.height)\n",
    "            final_image = composite_images(cropped_bg, transparent_image)\n",
    "\n",
    "            output_path = os.path.join(output_dir, transparent_image_name)\n",
    "            save_image(final_image, output_path)\n",
    "            logging.info(f\"Processed {transparent_image_name}\")\n",
    "\n",
    "        except ValueError as e:\n",
    "            logging.warning(f\"Skipping {transparent_image_name} due to small background: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {transparent_image_name}: {e}\")\n",
    "\n",
    "# You can call this function to process the images\n",
    "# process_images()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}